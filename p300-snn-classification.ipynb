{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import factorial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import AveragePooling1D, BatchNormalization, \\\n",
    "                                    Conv1D, Dense, Dropout, Flatten, LSTM\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import nengo\n",
    "import nengo_dl\n",
    "from nengo.utils.filter_design import cont2discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of cross-validation splits\n",
    "SPLITS = 5\n",
    "# size of train-test split\n",
    "SPLIT_SIZE = 0.8\n",
    "BATCH = 50\n",
    "EPOCHS = 30\n",
    "\n",
    "# True -- average random brainwave signal samples among people\n",
    "# False -- average single brainwave signal of one person (smoothening)\n",
    "AVERAGE_SAMPLES = True\n",
    "# when AVERAGE_SAMPLES is True, it is amount of samples \n",
    "#   from people, that will be averaged together\n",
    "# when AVERAGE_SAMPLES is False, it is a sliding window size\n",
    "#   used to smoothen one signal from one person\n",
    "AVERAGING_AMOUNT = [3, 6, 9, 12, 15]\n",
    "# useful only when AVERAGE_SAMPLES is True\n",
    "TRAIN_DATA_AMOUNT = 10000\n",
    "\n",
    "# set seed to ensure this experiment is reproducible\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# splitter for cross-validation\n",
    "shs = ShuffleSplit(n_splits=SPLITS, train_size=SPLIT_SIZE, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(results_final, filename='results'):\n",
    "    df = pd.DataFrame(results_final)\n",
    "    df.to_csv(f'{filename}.csv', index=False)\n",
    "    df.to_excel(f'{filename}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88082580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_final_dict():\n",
    "    return {\n",
    "        'Model': [],\n",
    "        'Averaging': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-score': [],\n",
    "        'ConfM-00': [],\n",
    "        'ConfM-01': [],\n",
    "        'ConfM-10': [],\n",
    "        'ConfM-11': [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9d521",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371b1dc",
   "metadata": {},
   "source": [
    "### Load and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matrix with P300 dataset\n",
    "mat = loadmat('VarekaGTNEpochs.mat')\n",
    "target_data, non_target_data = mat['allTargetData'], mat['allNonTargetData']\n",
    "\n",
    "# filtering needs to be done on both arrays separately,\n",
    "# because of averging later, but it is done only once in total\n",
    "\n",
    "# filter noise above 100 uV\n",
    "threshold = 100.0\n",
    "filter_target_data, filter_non_target_data = [], []\n",
    "# filter target data\n",
    "for i in range(target_data.shape[0]):\n",
    "    if np.max(np.abs(target_data[i])) <= threshold:\n",
    "        filter_target_data.append(target_data[i])\n",
    "# filter non-target data\n",
    "for i in range(non_target_data.shape[0]):\n",
    "    if np.max(np.abs(non_target_data[i])) <= threshold:\n",
    "        filter_non_target_data.append(non_target_data[i])\n",
    "\n",
    "# replace loaded data with filtered data\n",
    "target_data = np.array(filter_target_data)\n",
    "non_target_data = np.array(filter_non_target_data)\n",
    "\n",
    "print('Target data size:', target_data.shape)\n",
    "print('Non-target data size:', non_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ed00b",
   "metadata": {},
   "source": [
    "### Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averages_over_signal(data, window_size):\n",
    "    # NOTE:\n",
    "    # It would be possible to speed up this function with\n",
    "    # `np.average(sliding_window_view(data, window_size, axis=2), axis=-1)`\n",
    "    # but `numpy.lib.stride_tricks.sliding_window_view` is available since NumPy 1.20\n",
    "    # and NumPy 1.20 does not work with Tensorflow 2.5.0.\n",
    "    # Specifically tensor operations in NumPy with LSTM layers.\n",
    "    # `numpy.lib.stride_tricks.as_strided` is not memory safe and could damage data.\n",
    "    if window_size < 2:\n",
    "        return data\n",
    "    \n",
    "    averages = []\n",
    "    \n",
    "    # 1. samples\n",
    "    for i in range(data.shape[0]):\n",
    "        averaged.append([[],[],[]])\n",
    "        # 2. channels\n",
    "        for j in range(data.shape[1]):\n",
    "            # 3. features -- floating window\n",
    "            for k in range(data.shape[2] - window_size + 1):\n",
    "                averages[i][j].append(np.average(data[i][j][k:k+window_size]))\n",
    "    \n",
    "    return np.array(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d77b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divided by 2, because this function is called twice,\n",
    "# for target and for non-target data\n",
    "def get_averages_over_samples(data, amount_to_average, amount_new_data=TRAIN_DATA_AMOUNT//2):\n",
    "    if amount_to_average < 2:\n",
    "        return data\n",
    "\n",
    "    # get `amount_new_data` amount of vectors \n",
    "    # with length `amount_to_average`\n",
    "    random_choices = np.random.choice(\n",
    "        np.arange(data.shape[0]),\n",
    "        (amount_new_data, amount_to_average)\n",
    "    )\n",
    "    \n",
    "    averaged = []\n",
    "    # use vectors with random numbers to choose samples to average\n",
    "    for rchoice in random_choices:\n",
    "        tmp = []\n",
    "        for i in rchoice:\n",
    "            tmp.append(data[i])\n",
    "        averaged.append(np.average(tmp, axis=0))\n",
    " \n",
    "    return np.array(averaged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c2a90",
   "metadata": {},
   "source": [
    "### Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_labels(targets, non_targets):\n",
    "    # target numbers are labeled with vector [1, 0]\n",
    "    target_labels = np.tile(np.array([1, 0]), (targets.shape[0], 1))\n",
    "    # non-target numbers are labeled with vector [1, 0]\n",
    "    non_target_labels = np.tile(np.array([0, 1]), (non_targets.shape[0], 1))\n",
    "\n",
    "    # concatenate target and non-target sampels and labels\n",
    "    samples = np.concatenate((targets, non_targets))\n",
    "    labels = np.vstack((target_labels, non_target_labels))\n",
    "\n",
    "    # reshape to single vector and for correct inputs\n",
    "    samples = samples.reshape((samples.shape[0], 1, -1))\n",
    "\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d803c7d",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf075b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "def get_train_test_data(samples, labels):\n",
    "    x, x_test, y, y_test = train_test_split(\n",
    "        samples, labels, train_size=SPLIT_SIZE, random_state=SEED)\n",
    "    return x, x_test, y, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91234b4",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11196211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(predictions, y_test):\n",
    "    # normalize predictions and reality vectors\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    reality = np.argmax(y_test, axis=-1)[:predictions.shape[0]]\n",
    "    # calculate metrics\n",
    "    accuracy = metrics.accuracy_score(y_true=reality, y_pred=preds)\n",
    "    precision = metrics.precision_score(y_true=reality, y_pred=preds)\n",
    "    recall = metrics.recall_score(y_true=reality, y_pred=preds)\n",
    "    f1 = metrics.f1_score(y_true=reality, y_pred=preds)\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true=reality, y_pred=preds)\n",
    "\n",
    "    return [accuracy, precision, recall, f1, confusion_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbbfde",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598116b",
   "metadata": {},
   "source": [
    "### ANN Convolutional (model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f88523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_conv(inp_data):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=32, kernel_size=8, strides=4, \n",
    "               padding='same', activation=relu,\n",
    "               input_shape=(inp_data.shape[1], inp_data.shape[2])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=0.3, seed=SEED),\n",
    "        AveragePooling1D(pool_size=4, strides=1, padding='same'),\n",
    "        Flatten(),\n",
    "        Dense(64, activation=relu),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=0.4, seed=SEED),\n",
    "        Dense(2, activation=softmax),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=BinaryCrossentropy(), \n",
    "        metrics=['acc', 'mae', \n",
    "                 tf.keras.metrics.Recall(), \n",
    "                 tf.keras.metrics.Precision()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2be8b",
   "metadata": {},
   "source": [
    "### ANN Long Short-Term Memory (model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe87d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_lstm():\n",
    "    # NOTE: LSTM does not produce reproducible results\n",
    "    model = Sequential([\n",
    "        LSTM(units=32, activation=relu, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=0.3, seed=SEED),\n",
    "        LSTM(units=32, activation=relu),\n",
    "        Dense(64, activation=relu),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=0.4, seed=SEED),\n",
    "        Dense(2, activation=softmax),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=BinaryCrossentropy(), \n",
    "        metrics=['acc', 'mae', \n",
    "                 tf.keras.metrics.Recall(), \n",
    "                 tf.keras.metrics.Precision()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815335de",
   "metadata": {},
   "source": [
    "### Conversion of model 1 to spiking model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ann2snn(model):\n",
    "    converter = nengo_dl.Converter(\n",
    "        model=model,\n",
    "        swap_activations={\n",
    "            tf.keras.activations.relu: nengo.SpikingRectifiedLinear()\n",
    "        },\n",
    "        scale_firing_rates=3000,\n",
    "        synapse=0.1,\n",
    "    )\n",
    "    return converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56797086",
   "metadata": {},
   "source": [
    "### LMU cell & SNN LMU (model 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe435d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMUCell(nengo.Network):\n",
    "    \"\"\"Spiking version of LMU cell.\n",
    "    source: https://www.nengo.ai/nengo-dl/v3.3.0/examples/lmu.html (06, 2021)\n",
    "    \"\"\"\n",
    "    def __init__(self, units, order, theta, input_d, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # compute the A and B matrices according to the LMU's mathematical derivation\n",
    "        # (see the paper for details)\n",
    "        Q = np.arange(order, dtype=np.float64)\n",
    "        R = (2 * Q + 1)[:, None] / theta\n",
    "        j, i = np.meshgrid(Q, Q)\n",
    "\n",
    "        A = np.where(i < j, -1, (-1.0) ** (i - j + 1)) * R\n",
    "        B = (-1.0) ** Q[:, None] * R\n",
    "        C = np.ones((1, order))\n",
    "        D = np.zeros((1,))\n",
    "\n",
    "        A, B, _, _, _ = cont2discrete((A, B, C, D), dt=1.0, method=\"zoh\")\n",
    "\n",
    "        with self:\n",
    "            nengo_dl.configure_settings(trainable=None)\n",
    "\n",
    "            # create objects corresponding to the x/u/m/h variables in the above diagram\n",
    "            self.x = nengo.Node(size_in=input_d)\n",
    "            self.u = nengo.Node(size_in=1)\n",
    "            self.m = nengo.Node(size_in=order)\n",
    "            self.h = nengo_dl.TensorNode(tf.nn.tanh, shape_in=(units,), pass_time=False)\n",
    "\n",
    "            # compute u_t from the above diagram.\n",
    "            # note that setting synapse=0 (versus synapse=None) adds a one-timestep\n",
    "            # delay, so we can think of any connections with synapse=0 as representing\n",
    "            # value_{t-1}\n",
    "            nengo.Connection(\n",
    "                self.x, self.u, transform=np.ones((1, input_d)), synapse=None)\n",
    "            nengo.Connection(self.h, self.u, transform=np.zeros((1, units)), synapse=0)\n",
    "            nengo.Connection(self.m, self.u, transform=np.zeros((1, order)), synapse=0)\n",
    "\n",
    "            # compute m_t\n",
    "            # in this implementation we'll make A and B non-trainable, but they\n",
    "            # could also be optimized in the same way as the other parameters\n",
    "            conn_A = nengo.Connection(self.m, self.m, transform=A, synapse=0)\n",
    "            self.config[conn_A].trainable = False\n",
    "            conn_B = nengo.Connection(self.u, self.m, transform=B, synapse=None)\n",
    "            self.config[conn_B].trainable = False\n",
    "\n",
    "            # compute h_t\n",
    "            nengo.Connection(\n",
    "                self.x, self.h, transform=np.zeros((units, input_d)), synapse=None\n",
    "            )\n",
    "            nengo.Connection(\n",
    "                self.h, self.h, transform=np.zeros((units, units)), synapse=0)\n",
    "            nengo.Connection(\n",
    "                self.m,\n",
    "                self.h,\n",
    "                transform=nengo_dl.dists.Glorot(distribution=\"normal\"),\n",
    "                synapse=None,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_lmu(inp_data):\n",
    "    with nengo.Network(seed=SEED) as net:\n",
    "        # input node for data\n",
    "        inp = nengo.Node(np.ones(inp_data.shape[-1]))\n",
    "        # LMU cell\n",
    "        lmu1 = LMUCell(units=212, order=256, \n",
    "                      theta=inp_data.shape[-1], input_d=inp_data.shape[-1])\n",
    "        lmu2 = LMUCell(units=212, order=256, \n",
    "                      theta=inp_data.shape[-1], input_d=212)\n",
    "        # output node for probing result data\n",
    "        out = nengo.Node(size_in=2)\n",
    "\n",
    "        # input node is connected with LMU's `x` variable,\n",
    "        # where input vectors flow into\n",
    "        nengo.Connection(inp, lmu1.x, synapse=None)\n",
    "        # LMU's hidden state is kept in a variable `h`\n",
    "        # it is also an output connected to output node\n",
    "        nengo.Connection(lmu1.h, lmu2.x, \n",
    "                         transform=nengo_dl.dists.Glorot(), synapse=None)\n",
    "        nengo.Connection(lmu2.h, out, \n",
    "                         transform=nengo_dl.dists.Glorot(), synapse=None)\n",
    "\n",
    "        # probe for collecting data\n",
    "        p = nengo.Probe(target=out)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792380f5",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ab130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ann(model, x_train, y_train, x_val, y_val):\n",
    "    model.fit(\n",
    "        x=x_train, y=y_train, batch_size=BATCH, epochs=EPOCHS,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=[EarlyStopping(patience=5, verbose=1, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b266106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ann(model, x_test, y_test):\n",
    "    predictions = model.predict(x_test, batch_size=BATCH)\n",
    "    return get_metrics(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ecafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_snn(network, x_test, y_test):\n",
    "    # NOTE: SNN LMU is being predicted in `run_lmu` function immediatelly,\n",
    "    #       because the trained simulator is closed there, so training is lost\n",
    "    with nengo_dl.Simulator(network=network, minibatch_size=BATCH) as sim:\n",
    "        predictions = sim.predict(x_test)\n",
    "    # retrieve predictions matrix from Nengo object\n",
    "    predictions = np.array(list(predictions.values())[0])\n",
    "    return get_metrics(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aea1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lmu(network, x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "    with nengo_dl.Simulator(network=network, minibatch_size=BATCH) as sim:\n",
    "        sim.compile(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            metrics=['acc', 'mae'],\n",
    "        )\n",
    "        # matrices with labels for training and validation \n",
    "        # need to be reshaped to 3 dimensions, in order to fit the network\n",
    "        sim.fit(\n",
    "            x=x_train,\n",
    "            y=y_train.reshape((y_train.shape[0], 1, -1)), \n",
    "            validation_data=(x_val, y_val.reshape((y_val.shape[0], 1, -1))),\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[EarlyStopping(patience=5, verbose=1, restore_best_weights=True)],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # test SNN network\n",
    "        predictions = sim.predict(x_test)\n",
    "        # retrieve predictions matrix from Nengo object\n",
    "        predictions = np.array(list(predictions.values())[0])\n",
    "    \n",
    "    return get_metrics(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f67b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_simulation(x, x_test, y, y_test):\n",
    "    iteration = 1\n",
    "    models_metrics = {\n",
    "        'ann_conv': [],\n",
    "        'ann_lstm': [],\n",
    "        'snn_conv': [],\n",
    "        'snn_lmu': [],\n",
    "    }\n",
    "\n",
    "    for train_idx, val_idx in shs.split(x):\n",
    "        print(f'--- iteration: {iteration}/{SPLITS} ---')\n",
    "        iteration += 1\n",
    "        \n",
    "        # split data to train and validation set\n",
    "        x_train, y_train = x[train_idx], y[train_idx]\n",
    "        x_val, y_val = x[val_idx], y[val_idx]\n",
    "\n",
    "        # train ANN models\n",
    "        print('--- ANN conv')\n",
    "        ann_conv = fit_ann(get_model_conv(x), x_train, y_train, x_val, y_val)\n",
    "        print('--- ANN lstm')\n",
    "        ann_lstm = fit_ann(get_model_lstm(), x_train, y_train, x_val, y_val)\n",
    "        # convert trained ann_conv to spiking convolutional network\n",
    "        print('--- SNN conv')\n",
    "        snn_conv = convert_ann2snn(ann_conv)\n",
    "        # train SNN model and immediately test \n",
    "        print('--- SNN lmu')\n",
    "        snn_lmu_results = run_lmu(\n",
    "            get_model_lmu(x), x_train, y_train, x_val, y_val, x_test, y_test\n",
    "        )\n",
    "        \n",
    "        # test NN models\n",
    "        models_metrics['ann_conv'].append(predict_ann(ann_conv, x_test, y_test))\n",
    "        models_metrics['ann_lstm'].append(predict_ann(ann_lstm, x_test, y_test))\n",
    "        models_metrics['snn_conv'].append(predict_snn(snn_conv.net, x_test, y_test))\n",
    "        models_metrics['snn_lmu'].append(snn_lmu_results)\n",
    "\n",
    "        # reshape confusion matrices to 1D vector for easier averaging\n",
    "        models_metrics['ann_conv'][-1][-1] = np.squeeze(\n",
    "            models_metrics['ann_conv'][-1][-1].reshape(1, -1))\n",
    "        models_metrics['ann_lstm'][-1][-1] = np.squeeze(\n",
    "            models_metrics['ann_lstm'][-1][-1].reshape(1, -1))\n",
    "        models_metrics['snn_conv'][-1][-1] = np.squeeze(\n",
    "            models_metrics['snn_conv'][-1][-1].reshape(1, -1))\n",
    "        models_metrics['snn_lmu'][-1][-1] = np.squeeze(\n",
    "            models_metrics['snn_lmu'][-1][-1].reshape(1, -1))\n",
    "\n",
    "    # average all results from cross-validation\n",
    "    models_metrics['ann_conv'] = np.average(models_metrics['ann_conv'], axis=0)\n",
    "    models_metrics['ann_lstm'] = np.average(models_metrics['ann_lstm'], axis=0)\n",
    "    models_metrics['snn_conv'] = np.average(models_metrics['snn_conv'], axis=0)\n",
    "    models_metrics['snn_lmu'] = np.average(models_metrics['snn_lmu'], axis=0)\n",
    "\n",
    "    return models_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add14223",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = get_results_final_dict()\n",
    "\n",
    "for avg in AVERAGING_AMOUNT:\n",
    "    if AVERAGE_SAMPLES and avg > 1:\n",
    "        # averages from multiple samples\n",
    "        target_data_averaged = get_averages_over_samples(target_data, avg)\n",
    "        non_target_data_averaged = get_averages_over_samples(non_target_data, avg)\n",
    "    else:\n",
    "        print(f'> Averaging signals with amount {avg}')\n",
    "        # averages (smoothened) brain signals\n",
    "        target_data_averaged = get_averages_over_signal(target_data, avg)\n",
    "        non_target_data_averaged = get_averages_over_signal(non_target_data, avg)\n",
    "    \n",
    "    # create samples and their labels from averaged data\n",
    "    samples, labels = get_data_labels(target_data_averaged, non_target_data_averaged)\n",
    "    \n",
    "    if AVERAGE_SAMPLES and avg <= 1:\n",
    "        print('> Averaging amount is smaller than 2. '\n",
    "              'Proceeding with standard train-test data split')\n",
    "\n",
    "    # create train and test datasets\n",
    "    if AVERAGE_SAMPLES and avg > 1:\n",
    "        # when samples are averaged, the original data is used as a test dataset\n",
    "        samples_test, labels_test = get_data_labels(target_data, non_target_data)\n",
    "        x, x_test, y, y_test = samples, samples_test, labels, labels_test\n",
    "    else:\n",
    "        x, x_test, y, y_test = get_train_test_data(samples, labels)\n",
    "\n",
    "    print(f'----- AVERAGING AMOUNT: {avg} -----')\n",
    "    results = run_simulation(x, x_test, y, y_test)\n",
    "\n",
    "    # cache results from current window size run for later export\n",
    "    for model_name, res in results.items():\n",
    "        results_final['Model'].append(model_name)\n",
    "        results_final['Averaging'].append(avg)\n",
    "        results_final['Accuracy'].append(res[0])\n",
    "        results_final['Precision'].append(res[1])\n",
    "        results_final['Recall'].append(res[2])\n",
    "        results_final['F1-score'].append(res[3])\n",
    "        results_final['ConfM-00'].append(res[4][0])\n",
    "        results_final['ConfM-01'].append(res[4][1])\n",
    "        results_final['ConfM-10'].append(res[4][2])\n",
    "        results_final['ConfM-11'].append(res[4][3])\n",
    "\n",
    "export_data(results_final)\n",
    "\n",
    "print('--- DONE sample averaging ---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
